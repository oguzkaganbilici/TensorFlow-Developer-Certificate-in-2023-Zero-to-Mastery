{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow Part 2: Fine-Tuning\n",
        "\n",
        "> In the previous notebook, we covered transfer learning feature extraction. Now it's time to learn about a new kind of transfer learning: `Fine Tuning`."
      ],
      "metadata": {
        "id": "6AjRHFfvnc6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jfsMvhCkvQj",
        "outputId": "2ac5bba9-07de-40a6-ea05-ff3c0f8337f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 24 19:19:51 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check if we use the TPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating helpher functions\n",
        "\n",
        "> In previous notebooks, we have created a bunch of helper functions, now we could rewrite them all, however, this is tedious.\n",
        "\n",
        "So, it's a good idea to put functions you'll want to use again in a script you can download and import into your notebooks or elsewhere.\n",
        "\n",
        "We have done this for some of the functions we have used previously here:\n",
        "\n",
        "https://raw.githubusercontent.com/oguzkaganbilici/TensorFlow-Developer-Certificate-in-2023-Zero-to-Mastery/main/helper_functions.py\n"
      ],
      "metadata": {
        "id": "CGBujEB3oBBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/oguzkaganbilici/TensorFlow-Developer-Certificate-in-2023-Zero-to-Mastery/main/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkldUCayn4i5",
        "outputId": "993cf459-8af4-48e7-e6bf-a732a57e2fd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 19:19:52--  https://raw.githubusercontent.com/oguzkaganbilici/TensorFlow-Developer-Certificate-in-2023-Zero-to-Mastery/main/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2665 (2.6K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]   2.60K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-24 19:19:52 (41.3 MB/s) - ‘helper_functions.py’ saved [2665/2665]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import helper functions we're going to use in this notebook\n",
        "\n",
        "from helper_functions import create_tensorboard_callback, unzip_file\n",
        "from helper_functions import plot_loss_curves, walk_in_dir"
      ],
      "metadata": {
        "id": "7_J5-dPHr4FP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get some data\n",
        "\n",
        "> This time we're going to see how we can use the pre-trained models within `tf.keras.application` and apply them to our own problem (recognizing images of food)"
      ],
      "metadata": {
        "id": "sRv9L4-Q7NYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of training data of 10 classes of Food101\n",
        "\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_file(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOrxo2bwsEi3",
        "outputId": "da5e38a2-e2f8-42fd-f662-4e96e9ad5068"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 19:28:29--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.207, 142.250.107.207, 74.125.197.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   163MB/s    in 1.0s    \n",
            "\n",
            "2024-01-24 19:28:30 (163 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out how many images and subdirectories are in our dataset\n",
        "\n",
        "walk_in_dir(\"/content/10_food_classes_10_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDEA6UQf8frp",
        "outputId": "0ff59abb-56d3-40e8-b1f1-db1b60e58c40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 of directories and 0 images in /content/10_food_classes_10_percent.\n",
            "There are 10 of directories and 0 images in /content/10_food_classes_10_percent/test.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/hamburger.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/grilled_salmon.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/ramen.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/sushi.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/chicken_wings.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/pizza.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/steak.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/ice_cream.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/chicken_curry.\n",
            "There are 0 of directories and 250 images in /content/10_food_classes_10_percent/test/fried_rice.\n",
            "There are 10 of directories and 0 images in /content/10_food_classes_10_percent/train.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/hamburger.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/grilled_salmon.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/ramen.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/sushi.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/chicken_wings.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/pizza.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/steak.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/ice_cream.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/chicken_curry.\n",
            "There are 0 of directories and 75 images in /content/10_food_classes_10_percent/train/fried_rice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and testing dir\n",
        "\n",
        "train_dir = \"/content/10_food_classes_10_percent/train\"\n",
        "test_dir = \"/content/10_food_classes_10_percent/test\""
      ],
      "metadata": {
        "id": "DwCX0ksz8u9A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_10_percent = image_dataset_from_directory(directory=train_dir,\n",
        "                                                     image_size=IMG_SIZE,\n",
        "                                                     label_mode=\"categorical\",\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     )\n",
        "\n",
        "test_data = image_dataset_from_directory(directory=test_dir,\n",
        "                                         image_size=IMG_SIZE,\n",
        "                                         label_mode=\"categorical\",\n",
        "                                         batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7abwmS59Nyl",
        "outputId": "49f382c7-5152-495e-fdda-7735eaa68884"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAJR6ZNS_EXs",
        "outputId": "5edc0371-4c2f-434e-9352-0d9c55660e2c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create batches of 32 images of size 224x224 split into red, green, blue color channels."
      ],
      "metadata": {
        "id": "1JEUwq_x_wn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the clas names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqM2crl2_VD2",
        "outputId": "828339d0-4b16-4a96-8a79-9397e9dccdc2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YY5ki-BRBTxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}